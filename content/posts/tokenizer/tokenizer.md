---
title : 'Tokenizer'
date : 2024-11-22T16:59:52+08:00
draft : true
tags: ["LLM","Tokenizer","BPE"]
author: "Collin Liu"
# author: ["Me", "You"] # multiple authors
showToc: true
TocOpen: false
draft: false
hidemeta: false
comments: false
canonicalURL: "https://canonical.url/to/page"
disableHLJS: true # to disable highlightjs
disableShare: false
disableHLJS: false
hideSummary: false
searchHidden: false 
ShowReadingTime: true
ShowBreadCrumbs: true
ShowPostNavLinks: false
ShowWordCount: true
ShowRssButtonInSectionTermList: true
UseHugoToc: true
# cover:
   # image: "https://ia.samaltman.com/_next/image?url=%2Fimages%2Fcover.png&w=3840&q=75" # image path/url
   # alt: "<alt text>" # alt text
   # caption: "The Intelligence Age" # display caption under cover
   # relative: false # when using page bundles set this to true
    # hidden: false # only hide on current single page
# editPost:
#     URL: "https://github.com/<path_to_repo>/content"
#     Text: "Suggest Changes" # edit text
#     appendFilePath: true # to append file path to Edit link
---

## 1. BBPE tokenizer

- tokenizerå¸¸è§çš„è®­ç»ƒç®—æ³•æ˜¯bpe,è€Œç›®å‰å„ä¸ªä¼ä¸šéƒ½åœ¨ä½¿ç”¨BBPE
- BBPE(Byte-level Byte-Pair Encoding)æ˜¯ä»¥å­—èŠ‚ä¸ºæœ€å°å•ä½,è€ŒBPEæœ€æ—©åˆ™æ˜¯ä»¥ä¸€ä¸ªå­—ç¬¦ä¸ºæœ€å°å•å…ƒ

```
{'l o w </w>': 5, 'l o w e r </w>': 2, 'n e w e s t </w>': 6, 'w i d e s t </w>': 3}
```

```
{'l o w </w>': 5, 'l o w e r </w>': 2, 'n e w es t </w>': 6, 'w i d es t </w>': 3}
```

```
{'l o w </w>': 5, 'l o w e r </w>': 2, 'n e w est </w>': 6, 'w i d est </w>': 3}
```

- è¿™æ˜¯BPEçš„è®­ç»ƒæµç¨‹
  - å¯ä»¥çœ‹å‡ºå®ƒæ˜¯ä»¥ä¸€ä¸ªå­—æ¯ä¸ºæœ€å°å•ä½å¤„ç†
  - ç”±äºe s té¢‘æ¬¡æœ€å¤š -> est æœ€ç»ˆåˆå¹¶æˆä¸ºä¸€ä¸ªtoken
- è€ŒBBPEæ— éœ€æ˜¾å¼ç¼–ç ï¼Œç›´æ¥æ“ä½œå­—èŠ‚æµ
  - å°†æ¯ä¸ªå­—ç¬¦å¤„ç†æˆå­—èŠ‚æµå¼€å§‹è®­ç»ƒ 
  - æ¯”å¦‚è¿™é‡Œæ˜¯å°†textæ–‡æœ¬ä½¿ç”¨utf-8æ¥ç¼–ç æˆå­—èŠ‚æµ

```python
text = "ï¼µï½ï½‰ï½ƒï½ï½„ï½…! ğŸ…¤ğŸ…ğŸ…˜ğŸ…’ğŸ…ğŸ…“ğŸ…”â€½ ğŸ‡ºâ€ŒğŸ‡³â€ŒğŸ‡®â€ŒğŸ‡¨â€ŒğŸ‡´â€ŒğŸ‡©â€ŒğŸ‡ª! ğŸ˜„ The very name strikes fear and awe into the hearts of programmers worldwide. We all know we ought to â€œsupport Unicodeâ€ in our software (whatever that meansâ€”like using wchar_t for all the strings, right?). But Unicode can be abstruse, and diving into the thousand-page Unicode Standard plus its dozens of supplementary annexes, reports, and notes can be more than a little intimidating. I donâ€™t blame programmers for still finding the whole thing mysterious, even 30 years after Unicodeâ€™s inception."
tokens = text.encode("utf-8")
print(f"After utf-8 encode:{tokens}")
```

![image.png](../pics/pic.png)

- BPE vs BBPE
  - æœ€å°å•ä½
    - æ ‡å‡† BPE ä½¿ç”¨ **å­—ç¬¦ï¼ˆcharacterï¼‰** ä½œä¸ºæœ€å°åˆ†è¯å•ä½ã€‚æ¯ä¸ªè¾“å…¥æ–‡æœ¬è¢«åˆ†è§£æˆå­—ç¬¦ï¼ˆå¯¹äºä¸­æ–‡æ˜¯å•å­—ï¼Œå¯¹äºè‹±æ–‡æ˜¯å­—æ¯ï¼‰ã€‚
    - BBPE ä½¿ç”¨ **å­—èŠ‚ï¼ˆbyteï¼‰** ä½œä¸ºæœ€å°å•ä½ï¼Œè€Œä¸æ˜¯å­—ç¬¦ã€‚æ¯ä¸ªæ–‡æœ¬å…ˆè¢«ç¼–ç ä¸º UTF-8 å­—èŠ‚æµï¼Œç„¶åä»¥å­—èŠ‚ä¸ºå•ä½è¿›è¡Œ BPE æ“ä½œã€‚
  - BPEå±€é™æ€§
    - å¯¹äº Unicode æ–‡æœ¬ï¼ˆå¦‚ä¸­æ–‡ã€é˜¿æ‹‰ä¼¯è¯­ç­‰éæ‹‰ä¸å­—ç¬¦è¯­è¨€ï¼‰ï¼Œéœ€è¦æå‰å¤„ç†ã€‚
    - å¤šå­—èŠ‚å­—ç¬¦ï¼ˆå¦‚ä¸­æ–‡ï¼‰åœ¨åˆå§‹é˜¶æ®µä¼šç›´æ¥ä½œä¸ºä¸€ä¸ªå•ä½ï¼Œä¸ä¼šè¿›ä¸€æ­¥åˆ†å‰²ã€‚
  - BBPEä¼˜åŠ¿
    - æ”¯æŒä»»æ„è¯­è¨€ï¼Œæ— éœ€é¢„å¤„ç†ï¼ˆä¾‹å¦‚æ— éœ€é¢å¤–çš„åˆ†è¯å·¥å…·ï¼‰ã€‚
    - å¯¹äºæœªçŸ¥å­—ç¬¦æˆ–ç‰¹æ®Šç¬¦å·ï¼Œä¸ä¼šå› ä¸ºç¼ºä¹ç¼–ç è§„åˆ™è€Œå¤±è´¥ã€‚
    - æ”¯æŒæ··åˆè¯­è¨€æ–‡æœ¬ï¼ˆå¦‚ä¸­è‹±æ–‡æ··åˆã€å¸¦æœ‰è¡¨æƒ…ç¬¦å·çš„æ–‡æœ¬ï¼‰ã€‚

### 1.1 BPEä¸BBPEç®—æ³•æ€»ç»“

![image.png](../pics/pic01.png)

## 2. BBPE æºç è§£æ

- é¦–å…ˆéœ€è¦åŸºäº<code>huggingface</code>çš„<code>tokenizers</code>>åº“æ„å»ºä¸€ä¸ªè®­ç»ƒtokenizerçš„å‡½æ•°

```python
def train_bbpe_tokenizer(input_ds, lang, vocab_size=52000, save_path="bbpe_tokenizer_{0}"):
    """
    ä½¿ç”¨ Byte-Level BPE è®­ç»ƒä¸€ä¸ªæ”¯æŒä¸­è‹±æ–‡çš„åˆ†è¯å™¨ã€‚

    Args:
        input_ds: æ•°æ®é›†ï¼ŒåŒ…å«æ–‡æœ¬å¥å­çš„è¿­ä»£å™¨ã€‚
        lang: æ•°æ®é›†è¯­è¨€ã€‚
        vocab_size: è¯æ±‡è¡¨å¤§å°ã€‚
        save_path: åˆ†è¯å™¨ä¿å­˜è·¯å¾„ã€‚
    """
    # Step 1: Initialize the tokenizer
    tokenizer = Tokenizer(models.BPE())

    # Step 2: Customize pre-tokenization and decoding (Optional)
    # å½’ä¸€åŒ–ï¼ˆNFKC æ ‡å‡†åŒ–ï¼Œå¤„ç†å…¨è§’/åŠè§’å­—ç¬¦ç­‰é—®é¢˜ï¼‰
    tokenizer.normalizer = NFKC()

    # å­—èŠ‚çº§åˆ«çš„åˆ†è¯å™¨
    tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=False)

    # å­—èŠ‚çº§åˆ«çš„è§£ç å™¨
    tokenizer.decoder = decoders.ByteLevel()

    # Step 3: Train the tokenizer using the trainer
    trainer = trainers.BpeTrainer(
        vocab_size=vocab_size,
        # special_tokens=["<pad>", "<unk>", "<s>", "</s>", "<mask>"],  # ç‰¹æ®Š token
        special_tokens=["<|endoftext|>","<|padding|>"]
    )

    # ä»æ•°æ®é›†è¿­ä»£å™¨ä¸­è®­ç»ƒåˆ†è¯å™¨
    tokenizer.train_from_iterator(
        get_all_sentences(input_ds, lang), trainer=trainer
    )

    # Step 4: Save the tokenizer
    save_path = Path(save_path.format(lang))
    tokenizer.save(f"{save_path}.json", pretty=True)
    print(f"Tokenizer saved to {save_path}.json")

    return tokenizer

```

- æ„å»ºä¸€ä¸ªè¾…åŠ©å‡½æ•°,ç”¨æ¥æå–æ•°æ®é›†è¿›è¡Œè¿­ä»£å¼çš„è®­ç»ƒ

```python
def get_all_sentences(ds:Dataset,lang:str):
    for item in ds:
        yield item['translation'][lang]
```



## 3. åŠ è½½æ•°æ®é›†

- æ•°æ®é›†æˆ‘ä»¬ä½¿ç”¨```huggingface```çš„```datasets```

```python
from datasets import Dataset,load_dataset
from train_tokenizer import Config,get_all_sentences,get_or_train_tokenizer
config = Config()
config.datasource = 'Helsinki-NLP/opus-100'
config.lang_src =  'en'
config.lang_tgt =  'zh'
ds_raw = load_dataset(f"{config.datasource}", f"{config.lang_src}-{config.lang_tgt}",)
```

- æ­¤å¤–è¿™é‡Œå•ç‹¬æ„å»ºäº†ä¸€ä¸ª```Config```ç±»ç”¨æ¥é…ç½®è®­ç»ƒå‚æ•°

```python
@dataclass
class Config:
    datasource : str = 'opus_books'
    lang_src : str = 'en'
    lang_tgt : str = 'it'   
    tokenizer_file: str = 'tokenizer_{0}.json'
    unk_token: str = "[UNK]"
    special_tokens: list = ("[UNK]", "[PAD]", "[SOS]", "[EOS]")
    min_frequency: int = 2
    
# Optional
@dataclass
class BPEConfig:
    datasource : str = 'opus_books'
    lang_src : str = 'en'
    lang_tgt : str = 'zh'
    tokenizer_file: str = 'BPEtokenizer_{0}.json'
    vocab_size: int = 30000  # è¯æ±‡è¡¨å¤§å°
    min_frequency: int = 2   # å­è¯çš„æœ€å°é¢‘ç‡
    special_tokens: list = ("[UNK]", "[PAD]", "[SOS]", "[EOS]")
```

- æˆ‘ä»¬å¯ä»¥çœ‹ä¸€ä¸‹åŠ è½½çš„dataset's info

```python
ds_raw
```

![image.png](../pics/pic02.png)

## 4. Train a tokenizer based bbpe

- ç›´æ¥ä½¿ç”¨```train_bbpe_tokenizer```å‡½æ•°å¼€å§‹è®­ç»ƒ

```python
from train_HFtokenizer import train_bbpe_tokenizer
tokenizer=train_bbpe_tokenizer(input_ds=ds_raw['train'],vocab_size=30000,lang=config.lang_tgt) 
```

- è®­ç»ƒå®Œæˆä»¥åä¼šè¾“å‡º
  - ```Tokenizer saved to bbpe_tokenizer_zh.json```
- æˆ‘ä»¬ä»æ•°æ®é›†ä¸­ç®€å•æµ‹è¯•ä¸€ä¸‹

```python
text = get_all_sentences(ds_raw['train'],lang=config.lang_tgt)
text_iter= next(text)
text_iter
```

- Output: ```'ä¸Šå¸åœ¨æŒ‘æˆ˜ä½ ï¼Œä»–è¯´ä½ æ˜¯ç¬¨è›‹'```
- æˆ‘ä»¬åœ¨è¯¥æ–‡æœ¬è¿›è¡Œç¼–ç 

```python
tokens_zh = tokenizer.encode(text_iter)
print(f"ids:{tokens_zh.ids}")
print(f"type_ids:{tokens_zh.type_ids}") # type_idsä¸€èˆ¬ç”¨äºåŒºåˆ†å¥å­ç±»å‹ã€‚ä¾‹å¦‚ï¼Œåœ¨BERTä¸­ï¼Œtype_idsç”¨äºåŒºåˆ†å¥å­å¯¹ä¸­çš„ä¸¤ä¸ªå¥å­ï¼Œåˆ†åˆ«æ ‡è®°ä¸º0æˆ–1ã€‚åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œæ‰€æœ‰çš„type_idséƒ½æ˜¯0ï¼Œè¡¨æ˜è¿™æ˜¯ä¸€ä¸ªå•ç‹¬çš„å¥å­ã€‚
print(f"tokens:{tokens_zh.tokens}")
print(f"offsets:{tokens_zh.offsets}")
```

- Output

```python
ids:[3684, 241, 1756, 273, 13, 3609, 2180, 11477]
type_ids:[0, 0, 0, 0, 0, 0, 0, 0]
tokens:['Ã¤Â¸Ä¬Ã¥Â¸Ä¿', 'Ã¥Ä¾Â¨', 'Ã¦Ä®Ä³Ã¦ÄªÄº', 'Ã¤Â½Å‚', ',', 'Ã¤Â»Ä¸Ã¨Â¯Â´', 'Ã¤Â½Å‚Ã¦ÄºÂ¯', 'Ã§Â¬Â¨Ã¨Ä½Ä­']
offsets:[(0, 2), (2, 3), (3, 5), (5, 6), (6, 7), (7, 9), (9, 11), (11, 13)]
```

- å¯ä»¥å‘ç°èƒ½å¤Ÿæ­£å¸¸å°†æ–‡æœ¬æ˜ å°„æˆids
- ä½†æ˜¯tokensç¡®å®ä¹±ç å­˜å‚¨çš„,å½“æˆ‘ä»¬æŸ¥çœ‹è®­ç»ƒå®Œä¿å­˜çš„çš„```vocab.json```å‘ç°,å¾ˆå¤šå†…å®¹ä¹Ÿæ˜¯ä¹±ç çš„

![image.png](../pics/pic03.png)

### 4.1 ç°è±¡åˆ†æ

1. **Token æ˜¾ç¤ºä¸ºä¹±ç ï¼š**
   - åˆ†è¯å™¨çš„ `encode` æ–¹æ³•è¿”å›çš„ tokens çœ‹èµ·æ¥æ˜¯ä¹±ç ï¼ˆå¦‚ `Ã¡` ç­‰ï¼‰ã€‚
   - è¿™æ˜¯å› ä¸ºä½¿ç”¨äº† `ByteLevel` çš„åˆ†è¯å™¨ï¼Œå®ƒå°†è¾“å…¥æ–‡æœ¬æŒ‰å­—èŠ‚çº§åˆ«å¤„ç†ï¼Œæ¯ä¸ªå­—ç¬¦è¢«æ˜ å°„åˆ°å­—èŠ‚å½¢å¼ã€‚
2. **Decode åæ­£å¸¸è§£ç ï¼š**
   - åˆ†è¯å™¨çš„ `decode` æ–¹æ³•èƒ½å¤Ÿæ­£ç¡®åœ°å°† token IDs è½¬å›åŸå§‹æ–‡æœ¬ã€‚
   - è¿™æ˜¯å› ä¸º Byte-Level åˆ†è¯å™¨ä¼šåœ¨è§£ç æ—¶ï¼Œå°†è¿™äº›å­—èŠ‚å½¢å¼æ˜ å°„å›åŸå§‹çš„ Unicode å­—ç¬¦ã€‚

### 4.2 **ä¸ºä»€ä¹ˆä¼šå‡ºç°è¿™ç§ç°è±¡ï¼Ÿ**

Byte-Level BPE çš„æ ¸å¿ƒåŸç†æ˜¯å¯¹å­—èŠ‚åºåˆ—ï¼ˆè€Œä¸æ˜¯å­—ç¬¦ï¼‰è¿›è¡Œæ“ä½œï¼š

- **è®­ç»ƒæ—¶ï¼š** è¾“å…¥æ–‡æœ¬ä¼šè¢«åˆ†å‰²æˆå­—èŠ‚ï¼Œè€Œä¸æ˜¯ç›´æ¥æŒ‰å­—ç¬¦åˆ†å‰²ã€‚æ¯ä¸ªå­—èŠ‚éƒ½ä¼šè¢«æ˜ å°„åˆ°ä¸€ä¸ª tokenã€‚
- **ç¼–ç æ—¶ï¼š** è¾“å‡ºçš„ token æ˜¯å­—èŠ‚çš„ç¼–ç ç»“æœï¼Œå¯èƒ½æ— æ³•ç›´æ¥æ˜¾ç¤ºä¸ºäººç±»å¯è¯»çš„å­—ç¬¦ã€‚
- **è§£ç æ—¶ï¼š** åˆ†è¯å™¨ä¼šå°†è¿™äº›å­—èŠ‚è¿˜åŸä¸ºåŸå§‹æ–‡æœ¬ã€‚



## 5. ä½¿ç”¨tokenizerç›´æ¥decode

- é¢å¯¹ä¹±ç çš„token,å¯ä»¥ç›´æ¥ä½¿ç”¨tokenizerå»decodeæ–‡æœ¬çš„ids

```python
tokens_zh = tokenizer.decode(tokens_zh.ids)
tokens_zh
```

- Output: ```'ä¸Šå¸åœ¨æŒ‘æˆ˜ä½ ,ä»–è¯´ä½ æ˜¯ç¬¨è›‹'``
- å¯ä»¥çœ‹å‡ºèƒ½å¤Ÿç›´æ¥è¿”å›æ­£å¸¸äººç±»å¯ç†è§£çš„æ–‡æœ¬

- ä½†æ˜¯æˆ‘ä»¬æƒ³æ‰‹åŠ¨è§£ç å¯ä»¥å‚è€ƒKarpathyâ€˜s MinBPEæºç 

```python
def decode(self, ids):
     # given ids (list of integers), return Python string
    part_bytes = []
    for idx in ids:
        if idx in self.vocab:    # idx -> bytes   
            part_bytes.append(self.vocab[idx])  # éå†idsä¸­idx åœ¨vocabæ‰¾åˆ°å¯¹åº”çš„token utf-8è¡¨ç¤º
        elif idx in self.inverse_special_tokens: # å¦‚æœæ˜¯ç‰¹æ®Štokenå¯¹åº”çš„idx åœ¨å€’ç½®çš„special token å­—å…¸é‡ŒæŸ¥æ‰¾
            part_bytes.append(self.inverse_special_tokens[idx].encode("utf-8")) # ç„¶ååœ¨utf-8ç¼–ç è½¬åŒ–ä¸ºtoken
        else:
            raise ValueError(f"invalid token id: {idx}")
    text_bytes = b"".join(part_bytes)
    text = text_bytes.decode("utf-8", errors="replace") # å¯¹text_bytesè¿›è¡Œutf-8è§£ç å½¢æˆtoken
    return text
```

