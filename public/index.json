[{"content":"Tokenizer 实践 1. BBPE tokenizer tokenizer常见的训练算法是bpe,而目前各个企业都在使用BBPE BBPE(Byte-level Byte-Pair Encoding)是以字节为最小单位,而BPE最早则是以一个字符为最小单元 {\u0026#39;l o w \u0026lt;/w\u0026gt;\u0026#39;: 5, \u0026#39;l o w e r \u0026lt;/w\u0026gt;\u0026#39;: 2, \u0026#39;n e w e s t \u0026lt;/w\u0026gt;\u0026#39;: 6, \u0026#39;w i d e s t \u0026lt;/w\u0026gt;\u0026#39;: 3} {\u0026#39;l o w \u0026lt;/w\u0026gt;\u0026#39;: 5, \u0026#39;l o w e r \u0026lt;/w\u0026gt;\u0026#39;: 2, \u0026#39;n e w es t \u0026lt;/w\u0026gt;\u0026#39;: 6, \u0026#39;w i d es t \u0026lt;/w\u0026gt;\u0026#39;: 3} {\u0026#39;l o w \u0026lt;/w\u0026gt;\u0026#39;: 5, \u0026#39;l o w e r \u0026lt;/w\u0026gt;\u0026#39;: 2, \u0026#39;n e w est \u0026lt;/w\u0026gt;\u0026#39;: 6, \u0026#39;w i d est \u0026lt;/w\u0026gt;\u0026#39;: 3} 这是BPE的训练流程 可以看出它是以一个字母为最小单位处理 由于e s t频次最多 -\u0026gt; est 最终合并成为一个token 而BBPE无需显式编码，直接操作字节流 将每个字符处理成字节流开始训练 比如这里是将text文本使用utf-8来编码成字节流 text = \u0026#34;Ｕｎｉｃｏｄｅ! 🅤🅝🅘🅒🅞🅓🅔‽ 🇺‌🇳‌🇮‌🇨‌🇴‌🇩‌🇪! 😄 The very name strikes fear and awe into the hearts of programmers worldwide. We all know we ought to “support Unicode” in our software (whatever that means—like using wchar_t for all the strings, right?). But Unicode can be abstruse, and diving into the thousand-page Unicode Standard plus its dozens of supplementary annexes, reports, and notes can be more than a little intimidating. I don’t blame programmers for still finding the whole thing mysterious, even 30 years after Unicode’s inception.\u0026#34; tokens = text.encode(\u0026#34;utf-8\u0026#34;) print(f\u0026#34;After utf-8 encode:{tokens}\u0026#34;) BPE vs BBPE 最小单位 标准 BPE 使用 字符（character） 作为最小分词单位。每个输入文本被分解成字符（对于中文是单字，对于英文是字母）。 BBPE 使用 字节（byte） 作为最小单位，而不是字符。每个文本先被编码为 UTF-8 字节流，然后以字节为单位进行 BPE 操作。 BPE局限性 对于 Unicode 文本（如中文、阿拉伯语等非拉丁字符语言），需要提前处理。 多字节字符（如中文）在初始阶段会直接作为一个单位，不会进一步分割。 BBPE优势 支持任意语言，无需预处理（例如无需额外的分词工具）。 对于未知字符或特殊符号，不会因为缺乏编码规则而失败。 支持混合语言文本（如中英文混合、带有表情符号的文本）。 1.1 BPE与BBPE算法总结 2. BBPE 源码解析 首先需要基于huggingface的tokenizers\u0026gt;库构建一个训练tokenizer的函数 def train_bbpe_tokenizer(input_ds, lang, vocab_size=52000, save_path=\u0026#34;bbpe_tokenizer_{0}\u0026#34;): \u0026#34;\u0026#34;\u0026#34; 使用 Byte-Level BPE 训练一个支持中英文的分词器。 Args: input_ds: 数据集，包含文本句子的迭代器。 lang: 数据集语言。 vocab_size: 词汇表大小。 save_path: 分词器保存路径。 \u0026#34;\u0026#34;\u0026#34; # Step 1: Initialize the tokenizer tokenizer = Tokenizer(models.BPE()) # Step 2: Customize pre-tokenization and decoding (Optional) # 归一化（NFKC 标准化，处理全角/半角字符等问题） tokenizer.normalizer = NFKC() # 字节级别的分词器 tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=False) # 字节级别的解码器 tokenizer.decoder = decoders.ByteLevel() # Step 3: Train the tokenizer using the trainer trainer = trainers.BpeTrainer( vocab_size=vocab_size, # special_tokens=[\u0026#34;\u0026lt;pad\u0026gt;\u0026#34;, \u0026#34;\u0026lt;unk\u0026gt;\u0026#34;, \u0026#34;\u0026lt;s\u0026gt;\u0026#34;, \u0026#34;\u0026lt;/s\u0026gt;\u0026#34;, \u0026#34;\u0026lt;mask\u0026gt;\u0026#34;], # 特殊 token special_tokens=[\u0026#34;\u0026lt;|endoftext|\u0026gt;\u0026#34;,\u0026#34;\u0026lt;|padding|\u0026gt;\u0026#34;] ) # 从数据集迭代器中训练分词器 tokenizer.train_from_iterator( get_all_sentences(input_ds, lang), trainer=trainer ) # Step 4: Save the tokenizer save_path = Path(save_path.format(lang)) tokenizer.save(f\u0026#34;{save_path}.json\u0026#34;, pretty=True) print(f\u0026#34;Tokenizer saved to {save_path}.json\u0026#34;) return tokenizer 构建一个辅助函数,用来提取数据集进行迭代式的训练 def get_all_sentences(ds:Dataset,lang:str): for item in ds: yield item[\u0026#39;translation\u0026#39;][lang] 3. 加载数据集 数据集我们使用huggingface的datasets from datasets import Dataset,load_dataset from train_tokenizer import Config,get_all_sentences,get_or_train_tokenizer config = Config() config.datasource = \u0026#39;Helsinki-NLP/opus-100\u0026#39; config.lang_src = \u0026#39;en\u0026#39; config.lang_tgt = \u0026#39;zh\u0026#39; ds_raw = load_dataset(f\u0026#34;{config.datasource}\u0026#34;, f\u0026#34;{config.lang_src}-{config.lang_tgt}\u0026#34;,) 此外这里单独构建了一个Config类用来配置训练参数 @dataclass class Config: datasource : str = \u0026#39;opus_books\u0026#39; lang_src : str = \u0026#39;en\u0026#39; lang_tgt : str = \u0026#39;it\u0026#39; tokenizer_file: str = \u0026#39;tokenizer_{0}.json\u0026#39; unk_token: str = \u0026#34;[UNK]\u0026#34; special_tokens: list = (\u0026#34;[UNK]\u0026#34;, \u0026#34;[PAD]\u0026#34;, \u0026#34;[SOS]\u0026#34;, \u0026#34;[EOS]\u0026#34;) min_frequency: int = 2 # Optional @dataclass class BPEConfig: datasource : str = \u0026#39;opus_books\u0026#39; lang_src : str = \u0026#39;en\u0026#39; lang_tgt : str = \u0026#39;zh\u0026#39; tokenizer_file: str = \u0026#39;BPEtokenizer_{0}.json\u0026#39; vocab_size: int = 30000 # 词汇表大小 min_frequency: int = 2 # 子词的最小频率 special_tokens: list = (\u0026#34;[UNK]\u0026#34;, \u0026#34;[PAD]\u0026#34;, \u0026#34;[SOS]\u0026#34;, \u0026#34;[EOS]\u0026#34;) 我们可以看一下加载的dataset\u0026rsquo;s info ds_raw 4. Train a tokenizer based bbpe 直接使用train_bbpe_tokenizer函数开始训练 from train_HFtokenizer import train_bbpe_tokenizer tokenizer=train_bbpe_tokenizer(input_ds=ds_raw[\u0026#39;train\u0026#39;],vocab_size=30000,lang=config.lang_tgt) 训练完成以后会输出 Tokenizer saved to bbpe_tokenizer_zh.json 我们从数据集中简单测试一下 text = get_all_sentences(ds_raw[\u0026#39;train\u0026#39;],lang=config.lang_tgt) text_iter= next(text) text_iter Output: '上帝在挑战你，他说你是笨蛋' 我们在该文本进行编码 tokens_zh = tokenizer.encode(text_iter) print(f\u0026#34;ids:{tokens_zh.ids}\u0026#34;) print(f\u0026#34;type_ids:{tokens_zh.type_ids}\u0026#34;) # type_ids一般用于区分句子类型。例如，在BERT中，type_ids用于区分句子对中的两个句子，分别标记为0或1。在这个例子中，所有的type_ids都是0，表明这是一个单独的句子。 print(f\u0026#34;tokens:{tokens_zh.tokens}\u0026#34;) print(f\u0026#34;offsets:{tokens_zh.offsets}\u0026#34;) Output ids:[3684, 241, 1756, 273, 13, 3609, 2180, 11477] type_ids:[0, 0, 0, 0, 0, 0, 0, 0] tokens:[\u0026#39;ä¸Ĭå¸Ŀ\u0026#39;, \u0026#39;åľ¨\u0026#39;, \u0026#39;æĮĳæĪĺ\u0026#39;, \u0026#39;ä½ł\u0026#39;, \u0026#39;,\u0026#39;, \u0026#39;ä»ĸè¯´\u0026#39;, \u0026#39;ä½łæĺ¯\u0026#39;, \u0026#39;ç¬¨èĽĭ\u0026#39;] offsets:[(0, 2), (2, 3), (3, 5), (5, 6), (6, 7), (7, 9), (9, 11), (11, 13)] 可以发现能够正常将文本映射成ids 但是tokens确实乱码存储的,当我们查看训练完保存的的vocab.json发现,很多内容也是乱码的 4.1 现象分析 Token 显示为乱码： 分词器的 encode 方法返回的 tokens 看起来是乱码（如 á 等）。 这是因为使用了 ByteLevel 的分词器，它将输入文本按字节级别处理，每个字符被映射到字节形式。 Decode 后正常解码： 分词器的 decode 方法能够正确地将 token IDs 转回原始文本。 这是因为 Byte-Level 分词器会在解码时，将这些字节形式映射回原始的 Unicode 字符。 4.2 为什么会出现这种现象？ Byte-Level BPE 的核心原理是对字节序列（而不是字符）进行操作：\n训练时： 输入文本会被分割成字节，而不是直接按字符分割。每个字节都会被映射到一个 token。 编码时： 输出的 token 是字节的编码结果，可能无法直接显示为人类可读的字符。 解码时： 分词器会将这些字节还原为原始文本。 5. 使用tokenizer直接decode 面对乱码的token,可以直接使用tokenizer去decode文本的ids tokens_zh = tokenizer.decode(tokens_zh.ids) tokens_zh Output: ```\u0026lsquo;上帝在挑战你,他说你是笨蛋\u0026rsquo;``\n可以看出能够直接返回正常人类可理解的文本\n但是我们想手动解码可以参考Karpathy‘s MinBPE源码\ndef decode(self, ids): # given ids (list of integers), return Python string part_bytes = [] for idx in ids: if idx in self.vocab: # idx -\u0026gt; bytes part_bytes.append(self.vocab[idx]) # 遍历ids中idx 在vocab找到对应的token utf-8表示 elif idx in self.inverse_special_tokens: # 如果是特殊token对应的idx 在倒置的special token 字典里查找 part_bytes.append(self.inverse_special_tokens[idx].encode(\u0026#34;utf-8\u0026#34;)) # 然后在utf-8编码转化为token else: raise ValueError(f\u0026#34;invalid token id: {idx}\u0026#34;) text_bytes = b\u0026#34;\u0026#34;.join(part_bytes) text = text_bytes.decode(\u0026#34;utf-8\u0026#34;, errors=\u0026#34;replace\u0026#34;) # 对text_bytes进行utf-8解码形成token return text ","permalink":"http://localhost:1313/posts/tokenizer/tokenizer/","summary":"\u003ch1 id=\"tokenizer-实践\"\u003eTokenizer 实践\u003c/h1\u003e\n\u003ch2 id=\"1-bbpe-tokenizer\"\u003e1. BBPE tokenizer\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003etokenizer常见的训练算法是bpe,而目前各个企业都在使用BBPE\u003c/li\u003e\n\u003cli\u003eBBPE(Byte-level Byte-Pair Encoding)是以字节为最小单位,而BPE最早则是以一个字符为最小单元\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003e{\u0026#39;l o w \u0026lt;/w\u0026gt;\u0026#39;: 5, \u0026#39;l o w e r \u0026lt;/w\u0026gt;\u0026#39;: 2, \u0026#39;n e w e s t \u0026lt;/w\u0026gt;\u0026#39;: 6, \u0026#39;w i d e s t \u0026lt;/w\u0026gt;\u0026#39;: 3}\n\u003c/code\u003e\u003c/pre\u003e\u003cpre tabindex=\"0\"\u003e\u003ccode\u003e{\u0026#39;l o w \u0026lt;/w\u0026gt;\u0026#39;: 5, \u0026#39;l o w e r \u0026lt;/w\u0026gt;\u0026#39;: 2, \u0026#39;n e w es t \u0026lt;/w\u0026gt;\u0026#39;: 6, \u0026#39;w i d es t \u0026lt;/w\u0026gt;\u0026#39;: 3}\n\u003c/code\u003e\u003c/pre\u003e\u003cpre tabindex=\"0\"\u003e\u003ccode\u003e{\u0026#39;l o w \u0026lt;/w\u0026gt;\u0026#39;: 5, \u0026#39;l o w e r \u0026lt;/w\u0026gt;\u0026#39;: 2, \u0026#39;n e w est \u0026lt;/w\u0026gt;\u0026#39;: 6, \u0026#39;w i d est \u0026lt;/w\u0026gt;\u0026#39;: 3}\n\u003c/code\u003e\u003c/pre\u003e\u003cul\u003e\n\u003cli\u003e这是BPE的训练流程\n\u003cul\u003e\n\u003cli\u003e可以看出它是以一个字母为最小单位处理\u003c/li\u003e\n\u003cli\u003e由于e s t频次最多 -\u0026gt; est 最终合并成为一个token\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e而BBPE无需显式编码，直接操作字节流\n\u003cul\u003e\n\u003cli\u003e将每个字符处理成字节流开始训练\u003c/li\u003e\n\u003cli\u003e比如这里是将text文本使用utf-8来编码成字节流\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003etext\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;Ｕｎｉｃｏｄｅ! 🅤🅝🅘🅒🅞🅓🅔‽ 🇺‌🇳‌🇮‌🇨‌🇴‌🇩‌🇪! 😄 The very name strikes fear and awe into the hearts of programmers worldwide. We all know we ought to “support Unicode” in our software (whatever that means—like using wchar_t for all the strings, right?). But Unicode can be abstruse, and diving into the thousand-page Unicode Standard plus its dozens of supplementary annexes, reports, and notes can be more than a little intimidating. I don’t blame programmers for still finding the whole thing mysterious, even 30 years after Unicode’s inception.\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003etokens\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003etext\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eencode\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;utf-8\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"sa\"\u003ef\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;After utf-8 encode:\u003c/span\u003e\u003cspan class=\"si\"\u003e{\u003c/span\u003e\u003cspan class=\"n\"\u003etokens\u003c/span\u003e\u003cspan class=\"si\"\u003e}\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e\u003cimg loading=\"lazy\" src=\"../pics/pic.png\" alt=\"image.png\"  /\u003e\n\u003c/p\u003e","title":"Tokenizer"},{"content":"Git cd projectFile git confit --global user.name \u0026#34;Collin Liu\u0026#34; git confit --global user.email juunbailiu@gmail.com git init -- 初始化本地项目 git status -- 查看文件状态 git add -- 添加到缓存区 git commit -- 提交到本地仓库 touch .gitignore -- 添加不需要追踪的文件 git branch bad-boy git branch git checkout bad-boy -- 切换分支 trick： git commit -a -m “XXXX” git branch -D bad-boy --删除分支 git checkout -b tmp --创建并切换新分支 git merge temp -- 把temp版本的文件合并到当前分支 ","permalink":"http://localhost:1313/posts/gitee/","summary":"\u003ch1 id=\"git\"\u003eGit\u003c/h1\u003e\n\u003chr\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"./pics/image.png\" alt=\"image.png\"  /\u003e\n\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"nb\"\u003ecd\u003c/span\u003e projectFile\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003egit confit --global user.name \u003cspan class=\"s2\"\u003e\u0026#34;Collin Liu\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003egit confit --global user.email juunbailiu@gmail.com\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003egit init  -- 初始化本地项目\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003egit status  -- 查看文件状态\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003egit add     -- 添加到缓存区\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003egit commit  -- 提交到本地仓库\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003etouch .gitignore -- 添加不需要追踪的文件\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003egit branch bad-boy\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003egit branch \n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003egit checkout bad-boy -- 切换分支\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003etrick：\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\tgit commit -a -m “XXXX”\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003egit branch -D bad-boy --删除分支\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003egit checkout -b tmp   --创建并切换新分支\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003egit merge temp -- 把temp版本的文件合并到当前分支\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e","title":"Gitee"},{"content":"The Intelligence Age by Sam Altman Sam Altman explores the transformative power of AI in reshaping society. Key points include:\n1. AI’s Future Potential AI will drive significant advances in fields like healthcare, education, and scientific research. It promises to dramatically boost human productivity and quality of life.\n2. Challenges Ahead Equitable access to AI resources is crucial, requiring vast compute power and energy. There are potential risks, especially concerning job displacement in the labor market.\n3. Prosperity \u0026amp; Responsibility The coming AI age can create widespread prosperity if managed responsibly. There is a need for global collaboration to ensure benefits are shared equally.\n","permalink":"http://localhost:1313/posts/myfirstpost/","summary":"\u003ch1 id=\"the-intelligence-age-by-sam-altman\"\u003eThe Intelligence Age by Sam Altman\u003c/h1\u003e\n\u003cp\u003eSam Altman explores the transformative power of AI in reshaping society. Key points include:\u003c/p\u003e\n\u003ch2 id=\"1-ais-future-potential\"\u003e1. AI’s Future Potential\u003c/h2\u003e\n\u003cp\u003eAI will drive significant advances in fields like healthcare, education, and scientific research.\nIt promises to dramatically boost human productivity and quality of life.\u003c/p\u003e\n\u003ch2 id=\"2-challenges-ahead\"\u003e2. Challenges Ahead\u003c/h2\u003e\n\u003cp\u003eEquitable access to AI resources is crucial, requiring vast compute power and energy.\nThere are potential risks, especially concerning job displacement in the labor market.\u003c/p\u003e","title":"An introduction to Sam Altman's blog: The Intelligence Age"}]