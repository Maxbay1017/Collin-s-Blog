[{"content":"Tokenizer å®è·µ 1. BBPE tokenizer tokenizerå¸¸è§çš„è®­ç»ƒç®—æ³•æ˜¯bpe,è€Œç›®å‰å„ä¸ªä¼ä¸šéƒ½åœ¨ä½¿ç”¨BBPE BBPE(Byte-level Byte-Pair Encoding)æ˜¯ä»¥å­—èŠ‚ä¸ºæœ€å°å•ä½,è€ŒBPEæœ€æ—©åˆ™æ˜¯ä»¥ä¸€ä¸ªå­—ç¬¦ä¸ºæœ€å°å•å…ƒ {\u0026#39;l o w \u0026lt;/w\u0026gt;\u0026#39;: 5, \u0026#39;l o w e r \u0026lt;/w\u0026gt;\u0026#39;: 2, \u0026#39;n e w e s t \u0026lt;/w\u0026gt;\u0026#39;: 6, \u0026#39;w i d e s t \u0026lt;/w\u0026gt;\u0026#39;: 3} {\u0026#39;l o w \u0026lt;/w\u0026gt;\u0026#39;: 5, \u0026#39;l o w e r \u0026lt;/w\u0026gt;\u0026#39;: 2, \u0026#39;n e w es t \u0026lt;/w\u0026gt;\u0026#39;: 6, \u0026#39;w i d es t \u0026lt;/w\u0026gt;\u0026#39;: 3} {\u0026#39;l o w \u0026lt;/w\u0026gt;\u0026#39;: 5, \u0026#39;l o w e r \u0026lt;/w\u0026gt;\u0026#39;: 2, \u0026#39;n e w est \u0026lt;/w\u0026gt;\u0026#39;: 6, \u0026#39;w i d est \u0026lt;/w\u0026gt;\u0026#39;: 3} è¿™æ˜¯BPEçš„è®­ç»ƒæµç¨‹ å¯ä»¥çœ‹å‡ºå®ƒæ˜¯ä»¥ä¸€ä¸ªå­—æ¯ä¸ºæœ€å°å•ä½å¤„ç† ç”±äºe s té¢‘æ¬¡æœ€å¤š -\u0026gt; est æœ€ç»ˆåˆå¹¶æˆä¸ºä¸€ä¸ªtoken è€ŒBBPEæ— éœ€æ˜¾å¼ç¼–ç ï¼Œç›´æ¥æ“ä½œå­—èŠ‚æµ å°†æ¯ä¸ªå­—ç¬¦å¤„ç†æˆå­—èŠ‚æµå¼€å§‹è®­ç»ƒ æ¯”å¦‚è¿™é‡Œæ˜¯å°†textæ–‡æœ¬ä½¿ç”¨utf-8æ¥ç¼–ç æˆå­—èŠ‚æµ text = \u0026#34;ï¼µï½ï½‰ï½ƒï½ï½„ï½…! ğŸ…¤ğŸ…ğŸ…˜ğŸ…’ğŸ…ğŸ…“ğŸ…”â€½ ğŸ‡ºâ€ŒğŸ‡³â€ŒğŸ‡®â€ŒğŸ‡¨â€ŒğŸ‡´â€ŒğŸ‡©â€ŒğŸ‡ª! ğŸ˜„ The very name strikes fear and awe into the hearts of programmers worldwide. We all know we ought to â€œsupport Unicodeâ€ in our software (whatever that meansâ€”like using wchar_t for all the strings, right?). But Unicode can be abstruse, and diving into the thousand-page Unicode Standard plus its dozens of supplementary annexes, reports, and notes can be more than a little intimidating. I donâ€™t blame programmers for still finding the whole thing mysterious, even 30 years after Unicodeâ€™s inception.\u0026#34; tokens = text.encode(\u0026#34;utf-8\u0026#34;) print(f\u0026#34;After utf-8 encode:{tokens}\u0026#34;) BPE vs BBPE æœ€å°å•ä½ æ ‡å‡† BPE ä½¿ç”¨ å­—ç¬¦ï¼ˆcharacterï¼‰ ä½œä¸ºæœ€å°åˆ†è¯å•ä½ã€‚æ¯ä¸ªè¾“å…¥æ–‡æœ¬è¢«åˆ†è§£æˆå­—ç¬¦ï¼ˆå¯¹äºä¸­æ–‡æ˜¯å•å­—ï¼Œå¯¹äºè‹±æ–‡æ˜¯å­—æ¯ï¼‰ã€‚ BBPE ä½¿ç”¨ å­—èŠ‚ï¼ˆbyteï¼‰ ä½œä¸ºæœ€å°å•ä½ï¼Œè€Œä¸æ˜¯å­—ç¬¦ã€‚æ¯ä¸ªæ–‡æœ¬å…ˆè¢«ç¼–ç ä¸º UTF-8 å­—èŠ‚æµï¼Œç„¶åä»¥å­—èŠ‚ä¸ºå•ä½è¿›è¡Œ BPE æ“ä½œã€‚ BPEå±€é™æ€§ å¯¹äº Unicode æ–‡æœ¬ï¼ˆå¦‚ä¸­æ–‡ã€é˜¿æ‹‰ä¼¯è¯­ç­‰éæ‹‰ä¸å­—ç¬¦è¯­è¨€ï¼‰ï¼Œéœ€è¦æå‰å¤„ç†ã€‚ å¤šå­—èŠ‚å­—ç¬¦ï¼ˆå¦‚ä¸­æ–‡ï¼‰åœ¨åˆå§‹é˜¶æ®µä¼šç›´æ¥ä½œä¸ºä¸€ä¸ªå•ä½ï¼Œä¸ä¼šè¿›ä¸€æ­¥åˆ†å‰²ã€‚ BBPEä¼˜åŠ¿ æ”¯æŒä»»æ„è¯­è¨€ï¼Œæ— éœ€é¢„å¤„ç†ï¼ˆä¾‹å¦‚æ— éœ€é¢å¤–çš„åˆ†è¯å·¥å…·ï¼‰ã€‚ å¯¹äºæœªçŸ¥å­—ç¬¦æˆ–ç‰¹æ®Šç¬¦å·ï¼Œä¸ä¼šå› ä¸ºç¼ºä¹ç¼–ç è§„åˆ™è€Œå¤±è´¥ã€‚ æ”¯æŒæ··åˆè¯­è¨€æ–‡æœ¬ï¼ˆå¦‚ä¸­è‹±æ–‡æ··åˆã€å¸¦æœ‰è¡¨æƒ…ç¬¦å·çš„æ–‡æœ¬ï¼‰ã€‚ 1.1 BPEä¸BBPEç®—æ³•æ€»ç»“ 2. BBPE æºç è§£æ é¦–å…ˆéœ€è¦åŸºäºhuggingfaceçš„tokenizers\u0026gt;åº“æ„å»ºä¸€ä¸ªè®­ç»ƒtokenizerçš„å‡½æ•° def train_bbpe_tokenizer(input_ds, lang, vocab_size=52000, save_path=\u0026#34;bbpe_tokenizer_{0}\u0026#34;): \u0026#34;\u0026#34;\u0026#34; ä½¿ç”¨ Byte-Level BPE è®­ç»ƒä¸€ä¸ªæ”¯æŒä¸­è‹±æ–‡çš„åˆ†è¯å™¨ã€‚ Args: input_ds: æ•°æ®é›†ï¼ŒåŒ…å«æ–‡æœ¬å¥å­çš„è¿­ä»£å™¨ã€‚ lang: æ•°æ®é›†è¯­è¨€ã€‚ vocab_size: è¯æ±‡è¡¨å¤§å°ã€‚ save_path: åˆ†è¯å™¨ä¿å­˜è·¯å¾„ã€‚ \u0026#34;\u0026#34;\u0026#34; # Step 1: Initialize the tokenizer tokenizer = Tokenizer(models.BPE()) # Step 2: Customize pre-tokenization and decoding (Optional) # å½’ä¸€åŒ–ï¼ˆNFKC æ ‡å‡†åŒ–ï¼Œå¤„ç†å…¨è§’/åŠè§’å­—ç¬¦ç­‰é—®é¢˜ï¼‰ tokenizer.normalizer = NFKC() # å­—èŠ‚çº§åˆ«çš„åˆ†è¯å™¨ tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=False) # å­—èŠ‚çº§åˆ«çš„è§£ç å™¨ tokenizer.decoder = decoders.ByteLevel() # Step 3: Train the tokenizer using the trainer trainer = trainers.BpeTrainer( vocab_size=vocab_size, # special_tokens=[\u0026#34;\u0026lt;pad\u0026gt;\u0026#34;, \u0026#34;\u0026lt;unk\u0026gt;\u0026#34;, \u0026#34;\u0026lt;s\u0026gt;\u0026#34;, \u0026#34;\u0026lt;/s\u0026gt;\u0026#34;, \u0026#34;\u0026lt;mask\u0026gt;\u0026#34;], # ç‰¹æ®Š token special_tokens=[\u0026#34;\u0026lt;|endoftext|\u0026gt;\u0026#34;,\u0026#34;\u0026lt;|padding|\u0026gt;\u0026#34;] ) # ä»æ•°æ®é›†è¿­ä»£å™¨ä¸­è®­ç»ƒåˆ†è¯å™¨ tokenizer.train_from_iterator( get_all_sentences(input_ds, lang), trainer=trainer ) # Step 4: Save the tokenizer save_path = Path(save_path.format(lang)) tokenizer.save(f\u0026#34;{save_path}.json\u0026#34;, pretty=True) print(f\u0026#34;Tokenizer saved to {save_path}.json\u0026#34;) return tokenizer æ„å»ºä¸€ä¸ªè¾…åŠ©å‡½æ•°,ç”¨æ¥æå–æ•°æ®é›†è¿›è¡Œè¿­ä»£å¼çš„è®­ç»ƒ def get_all_sentences(ds:Dataset,lang:str): for item in ds: yield item[\u0026#39;translation\u0026#39;][lang] 3. åŠ è½½æ•°æ®é›† æ•°æ®é›†æˆ‘ä»¬ä½¿ç”¨huggingfaceçš„datasets from datasets import Dataset,load_dataset from train_tokenizer import Config,get_all_sentences,get_or_train_tokenizer config = Config() config.datasource = \u0026#39;Helsinki-NLP/opus-100\u0026#39; config.lang_src = \u0026#39;en\u0026#39; config.lang_tgt = \u0026#39;zh\u0026#39; ds_raw = load_dataset(f\u0026#34;{config.datasource}\u0026#34;, f\u0026#34;{config.lang_src}-{config.lang_tgt}\u0026#34;,) æ­¤å¤–è¿™é‡Œå•ç‹¬æ„å»ºäº†ä¸€ä¸ªConfigç±»ç”¨æ¥é…ç½®è®­ç»ƒå‚æ•° @dataclass class Config: datasource : str = \u0026#39;opus_books\u0026#39; lang_src : str = \u0026#39;en\u0026#39; lang_tgt : str = \u0026#39;it\u0026#39; tokenizer_file: str = \u0026#39;tokenizer_{0}.json\u0026#39; unk_token: str = \u0026#34;[UNK]\u0026#34; special_tokens: list = (\u0026#34;[UNK]\u0026#34;, \u0026#34;[PAD]\u0026#34;, \u0026#34;[SOS]\u0026#34;, \u0026#34;[EOS]\u0026#34;) min_frequency: int = 2 # Optional @dataclass class BPEConfig: datasource : str = \u0026#39;opus_books\u0026#39; lang_src : str = \u0026#39;en\u0026#39; lang_tgt : str = \u0026#39;zh\u0026#39; tokenizer_file: str = \u0026#39;BPEtokenizer_{0}.json\u0026#39; vocab_size: int = 30000 # è¯æ±‡è¡¨å¤§å° min_frequency: int = 2 # å­è¯çš„æœ€å°é¢‘ç‡ special_tokens: list = (\u0026#34;[UNK]\u0026#34;, \u0026#34;[PAD]\u0026#34;, \u0026#34;[SOS]\u0026#34;, \u0026#34;[EOS]\u0026#34;) æˆ‘ä»¬å¯ä»¥çœ‹ä¸€ä¸‹åŠ è½½çš„dataset\u0026rsquo;s info ds_raw 4. Train a tokenizer based bbpe ç›´æ¥ä½¿ç”¨train_bbpe_tokenizerå‡½æ•°å¼€å§‹è®­ç»ƒ from train_HFtokenizer import train_bbpe_tokenizer tokenizer=train_bbpe_tokenizer(input_ds=ds_raw[\u0026#39;train\u0026#39;],vocab_size=30000,lang=config.lang_tgt) è®­ç»ƒå®Œæˆä»¥åä¼šè¾“å‡º Tokenizer saved to bbpe_tokenizer_zh.json æˆ‘ä»¬ä»æ•°æ®é›†ä¸­ç®€å•æµ‹è¯•ä¸€ä¸‹ text = get_all_sentences(ds_raw[\u0026#39;train\u0026#39;],lang=config.lang_tgt) text_iter= next(text) text_iter Output: 'ä¸Šå¸åœ¨æŒ‘æˆ˜ä½ ï¼Œä»–è¯´ä½ æ˜¯ç¬¨è›‹' æˆ‘ä»¬åœ¨è¯¥æ–‡æœ¬è¿›è¡Œç¼–ç  tokens_zh = tokenizer.encode(text_iter) print(f\u0026#34;ids:{tokens_zh.ids}\u0026#34;) print(f\u0026#34;type_ids:{tokens_zh.type_ids}\u0026#34;) # type_idsä¸€èˆ¬ç”¨äºåŒºåˆ†å¥å­ç±»å‹ã€‚ä¾‹å¦‚ï¼Œåœ¨BERTä¸­ï¼Œtype_idsç”¨äºåŒºåˆ†å¥å­å¯¹ä¸­çš„ä¸¤ä¸ªå¥å­ï¼Œåˆ†åˆ«æ ‡è®°ä¸º0æˆ–1ã€‚åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œæ‰€æœ‰çš„type_idséƒ½æ˜¯0ï¼Œè¡¨æ˜è¿™æ˜¯ä¸€ä¸ªå•ç‹¬çš„å¥å­ã€‚ print(f\u0026#34;tokens:{tokens_zh.tokens}\u0026#34;) print(f\u0026#34;offsets:{tokens_zh.offsets}\u0026#34;) Output ids:[3684, 241, 1756, 273, 13, 3609, 2180, 11477] type_ids:[0, 0, 0, 0, 0, 0, 0, 0] tokens:[\u0026#39;Ã¤Â¸Ä¬Ã¥Â¸Ä¿\u0026#39;, \u0026#39;Ã¥Ä¾Â¨\u0026#39;, \u0026#39;Ã¦Ä®Ä³Ã¦ÄªÄº\u0026#39;, \u0026#39;Ã¤Â½Å‚\u0026#39;, \u0026#39;,\u0026#39;, \u0026#39;Ã¤Â»Ä¸Ã¨Â¯Â´\u0026#39;, \u0026#39;Ã¤Â½Å‚Ã¦ÄºÂ¯\u0026#39;, \u0026#39;Ã§Â¬Â¨Ã¨Ä½Ä­\u0026#39;] offsets:[(0, 2), (2, 3), (3, 5), (5, 6), (6, 7), (7, 9), (9, 11), (11, 13)] å¯ä»¥å‘ç°èƒ½å¤Ÿæ­£å¸¸å°†æ–‡æœ¬æ˜ å°„æˆids ä½†æ˜¯tokensç¡®å®ä¹±ç å­˜å‚¨çš„,å½“æˆ‘ä»¬æŸ¥çœ‹è®­ç»ƒå®Œä¿å­˜çš„çš„vocab.jsonå‘ç°,å¾ˆå¤šå†…å®¹ä¹Ÿæ˜¯ä¹±ç çš„ 4.1 ç°è±¡åˆ†æ Token æ˜¾ç¤ºä¸ºä¹±ç ï¼š åˆ†è¯å™¨çš„ encode æ–¹æ³•è¿”å›çš„ tokens çœ‹èµ·æ¥æ˜¯ä¹±ç ï¼ˆå¦‚ Ã¡ ç­‰ï¼‰ã€‚ è¿™æ˜¯å› ä¸ºä½¿ç”¨äº† ByteLevel çš„åˆ†è¯å™¨ï¼Œå®ƒå°†è¾“å…¥æ–‡æœ¬æŒ‰å­—èŠ‚çº§åˆ«å¤„ç†ï¼Œæ¯ä¸ªå­—ç¬¦è¢«æ˜ å°„åˆ°å­—èŠ‚å½¢å¼ã€‚ Decode åæ­£å¸¸è§£ç ï¼š åˆ†è¯å™¨çš„ decode æ–¹æ³•èƒ½å¤Ÿæ­£ç¡®åœ°å°† token IDs è½¬å›åŸå§‹æ–‡æœ¬ã€‚ è¿™æ˜¯å› ä¸º Byte-Level åˆ†è¯å™¨ä¼šåœ¨è§£ç æ—¶ï¼Œå°†è¿™äº›å­—èŠ‚å½¢å¼æ˜ å°„å›åŸå§‹çš„ Unicode å­—ç¬¦ã€‚ 4.2 ä¸ºä»€ä¹ˆä¼šå‡ºç°è¿™ç§ç°è±¡ï¼Ÿ Byte-Level BPE çš„æ ¸å¿ƒåŸç†æ˜¯å¯¹å­—èŠ‚åºåˆ—ï¼ˆè€Œä¸æ˜¯å­—ç¬¦ï¼‰è¿›è¡Œæ“ä½œï¼š\nè®­ç»ƒæ—¶ï¼š è¾“å…¥æ–‡æœ¬ä¼šè¢«åˆ†å‰²æˆå­—èŠ‚ï¼Œè€Œä¸æ˜¯ç›´æ¥æŒ‰å­—ç¬¦åˆ†å‰²ã€‚æ¯ä¸ªå­—èŠ‚éƒ½ä¼šè¢«æ˜ å°„åˆ°ä¸€ä¸ª tokenã€‚ ç¼–ç æ—¶ï¼š è¾“å‡ºçš„ token æ˜¯å­—èŠ‚çš„ç¼–ç ç»“æœï¼Œå¯èƒ½æ— æ³•ç›´æ¥æ˜¾ç¤ºä¸ºäººç±»å¯è¯»çš„å­—ç¬¦ã€‚ è§£ç æ—¶ï¼š åˆ†è¯å™¨ä¼šå°†è¿™äº›å­—èŠ‚è¿˜åŸä¸ºåŸå§‹æ–‡æœ¬ã€‚ 5. ä½¿ç”¨tokenizerç›´æ¥decode é¢å¯¹ä¹±ç çš„token,å¯ä»¥ç›´æ¥ä½¿ç”¨tokenizerå»decodeæ–‡æœ¬çš„ids tokens_zh = tokenizer.decode(tokens_zh.ids) tokens_zh Output: ```\u0026lsquo;ä¸Šå¸åœ¨æŒ‘æˆ˜ä½ ,ä»–è¯´ä½ æ˜¯ç¬¨è›‹\u0026rsquo;``\nå¯ä»¥çœ‹å‡ºèƒ½å¤Ÿç›´æ¥è¿”å›æ­£å¸¸äººç±»å¯ç†è§£çš„æ–‡æœ¬\nä½†æ˜¯æˆ‘ä»¬æƒ³æ‰‹åŠ¨è§£ç å¯ä»¥å‚è€ƒKarpathyâ€˜s MinBPEæºç \ndef decode(self, ids): # given ids (list of integers), return Python string part_bytes = [] for idx in ids: if idx in self.vocab: # idx -\u0026gt; bytes part_bytes.append(self.vocab[idx]) # éå†idsä¸­idx åœ¨vocabæ‰¾åˆ°å¯¹åº”çš„token utf-8è¡¨ç¤º elif idx in self.inverse_special_tokens: # å¦‚æœæ˜¯ç‰¹æ®Štokenå¯¹åº”çš„idx åœ¨å€’ç½®çš„special token å­—å…¸é‡ŒæŸ¥æ‰¾ part_bytes.append(self.inverse_special_tokens[idx].encode(\u0026#34;utf-8\u0026#34;)) # ç„¶ååœ¨utf-8ç¼–ç è½¬åŒ–ä¸ºtoken else: raise ValueError(f\u0026#34;invalid token id: {idx}\u0026#34;) text_bytes = b\u0026#34;\u0026#34;.join(part_bytes) text = text_bytes.decode(\u0026#34;utf-8\u0026#34;, errors=\u0026#34;replace\u0026#34;) # å¯¹text_bytesè¿›è¡Œutf-8è§£ç å½¢æˆtoken return text ","permalink":"http://localhost:1313/posts/tokenizer/tokenizer/","summary":"\u003ch1 id=\"tokenizer-å®è·µ\"\u003eTokenizer å®è·µ\u003c/h1\u003e\n\u003ch2 id=\"1-bbpe-tokenizer\"\u003e1. BBPE tokenizer\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003etokenizerå¸¸è§çš„è®­ç»ƒç®—æ³•æ˜¯bpe,è€Œç›®å‰å„ä¸ªä¼ä¸šéƒ½åœ¨ä½¿ç”¨BBPE\u003c/li\u003e\n\u003cli\u003eBBPE(Byte-level Byte-Pair Encoding)æ˜¯ä»¥å­—èŠ‚ä¸ºæœ€å°å•ä½,è€ŒBPEæœ€æ—©åˆ™æ˜¯ä»¥ä¸€ä¸ªå­—ç¬¦ä¸ºæœ€å°å•å…ƒ\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003e{\u0026#39;l o w \u0026lt;/w\u0026gt;\u0026#39;: 5, \u0026#39;l o w e r \u0026lt;/w\u0026gt;\u0026#39;: 2, \u0026#39;n e w e s t \u0026lt;/w\u0026gt;\u0026#39;: 6, \u0026#39;w i d e s t \u0026lt;/w\u0026gt;\u0026#39;: 3}\n\u003c/code\u003e\u003c/pre\u003e\u003cpre tabindex=\"0\"\u003e\u003ccode\u003e{\u0026#39;l o w \u0026lt;/w\u0026gt;\u0026#39;: 5, \u0026#39;l o w e r \u0026lt;/w\u0026gt;\u0026#39;: 2, \u0026#39;n e w es t \u0026lt;/w\u0026gt;\u0026#39;: 6, \u0026#39;w i d es t \u0026lt;/w\u0026gt;\u0026#39;: 3}\n\u003c/code\u003e\u003c/pre\u003e\u003cpre tabindex=\"0\"\u003e\u003ccode\u003e{\u0026#39;l o w \u0026lt;/w\u0026gt;\u0026#39;: 5, \u0026#39;l o w e r \u0026lt;/w\u0026gt;\u0026#39;: 2, \u0026#39;n e w est \u0026lt;/w\u0026gt;\u0026#39;: 6, \u0026#39;w i d est \u0026lt;/w\u0026gt;\u0026#39;: 3}\n\u003c/code\u003e\u003c/pre\u003e\u003cul\u003e\n\u003cli\u003eè¿™æ˜¯BPEçš„è®­ç»ƒæµç¨‹\n\u003cul\u003e\n\u003cli\u003eå¯ä»¥çœ‹å‡ºå®ƒæ˜¯ä»¥ä¸€ä¸ªå­—æ¯ä¸ºæœ€å°å•ä½å¤„ç†\u003c/li\u003e\n\u003cli\u003eç”±äºe s té¢‘æ¬¡æœ€å¤š -\u0026gt; est æœ€ç»ˆåˆå¹¶æˆä¸ºä¸€ä¸ªtoken\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eè€ŒBBPEæ— éœ€æ˜¾å¼ç¼–ç ï¼Œç›´æ¥æ“ä½œå­—èŠ‚æµ\n\u003cul\u003e\n\u003cli\u003eå°†æ¯ä¸ªå­—ç¬¦å¤„ç†æˆå­—èŠ‚æµå¼€å§‹è®­ç»ƒ\u003c/li\u003e\n\u003cli\u003eæ¯”å¦‚è¿™é‡Œæ˜¯å°†textæ–‡æœ¬ä½¿ç”¨utf-8æ¥ç¼–ç æˆå­—èŠ‚æµ\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003etext\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;ï¼µï½ï½‰ï½ƒï½ï½„ï½…! ğŸ…¤ğŸ…ğŸ…˜ğŸ…’ğŸ…ğŸ…“ğŸ…”â€½ ğŸ‡ºâ€ŒğŸ‡³â€ŒğŸ‡®â€ŒğŸ‡¨â€ŒğŸ‡´â€ŒğŸ‡©â€ŒğŸ‡ª! ğŸ˜„ The very name strikes fear and awe into the hearts of programmers worldwide. We all know we ought to â€œsupport Unicodeâ€ in our software (whatever that meansâ€”like using wchar_t for all the strings, right?). But Unicode can be abstruse, and diving into the thousand-page Unicode Standard plus its dozens of supplementary annexes, reports, and notes can be more than a little intimidating. I donâ€™t blame programmers for still finding the whole thing mysterious, even 30 years after Unicodeâ€™s inception.\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003etokens\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003etext\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eencode\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;utf-8\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"sa\"\u003ef\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;After utf-8 encode:\u003c/span\u003e\u003cspan class=\"si\"\u003e{\u003c/span\u003e\u003cspan class=\"n\"\u003etokens\u003c/span\u003e\u003cspan class=\"si\"\u003e}\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003e\u003cimg loading=\"lazy\" src=\"../pics/pic.png\" alt=\"image.png\"  /\u003e\n\u003c/p\u003e","title":"Tokenizer"},{"content":"Git cd projectFile git confit --global user.name \u0026#34;Collin Liu\u0026#34; git confit --global user.email juunbailiu@gmail.com git init -- åˆå§‹åŒ–æœ¬åœ°é¡¹ç›® git status -- æŸ¥çœ‹æ–‡ä»¶çŠ¶æ€ git add -- æ·»åŠ åˆ°ç¼“å­˜åŒº git commit -- æäº¤åˆ°æœ¬åœ°ä»“åº“ touch .gitignore -- æ·»åŠ ä¸éœ€è¦è¿½è¸ªçš„æ–‡ä»¶ git branch bad-boy git branch git checkout bad-boy -- åˆ‡æ¢åˆ†æ”¯ trickï¼š git commit -a -m â€œXXXXâ€ git branch -D bad-boy --åˆ é™¤åˆ†æ”¯ git checkout -b tmp --åˆ›å»ºå¹¶åˆ‡æ¢æ–°åˆ†æ”¯ git merge temp -- æŠŠtempç‰ˆæœ¬çš„æ–‡ä»¶åˆå¹¶åˆ°å½“å‰åˆ†æ”¯ ","permalink":"http://localhost:1313/posts/gitee/","summary":"\u003ch1 id=\"git\"\u003eGit\u003c/h1\u003e\n\u003chr\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"./pics/image.png\" alt=\"image.png\"  /\u003e\n\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"nb\"\u003ecd\u003c/span\u003e projectFile\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003egit confit --global user.name \u003cspan class=\"s2\"\u003e\u0026#34;Collin Liu\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003egit confit --global user.email juunbailiu@gmail.com\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003egit init  -- åˆå§‹åŒ–æœ¬åœ°é¡¹ç›®\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003egit status  -- æŸ¥çœ‹æ–‡ä»¶çŠ¶æ€\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003egit add     -- æ·»åŠ åˆ°ç¼“å­˜åŒº\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003egit commit  -- æäº¤åˆ°æœ¬åœ°ä»“åº“\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003etouch .gitignore -- æ·»åŠ ä¸éœ€è¦è¿½è¸ªçš„æ–‡ä»¶\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003egit branch bad-boy\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003egit branch \n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003egit checkout bad-boy -- åˆ‡æ¢åˆ†æ”¯\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003etrickï¼š\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\tgit commit -a -m â€œXXXXâ€\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003egit branch -D bad-boy --åˆ é™¤åˆ†æ”¯\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003egit checkout -b tmp   --åˆ›å»ºå¹¶åˆ‡æ¢æ–°åˆ†æ”¯\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003egit merge temp -- æŠŠtempç‰ˆæœ¬çš„æ–‡ä»¶åˆå¹¶åˆ°å½“å‰åˆ†æ”¯\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e","title":"Gitee"},{"content":"The Intelligence Age by Sam Altman Sam Altman explores the transformative power of AI in reshaping society. Key points include:\n1. AIâ€™s Future Potential AI will drive significant advances in fields like healthcare, education, and scientific research. It promises to dramatically boost human productivity and quality of life.\n2. Challenges Ahead Equitable access to AI resources is crucial, requiring vast compute power and energy. There are potential risks, especially concerning job displacement in the labor market.\n3. Prosperity \u0026amp; Responsibility The coming AI age can create widespread prosperity if managed responsibly. There is a need for global collaboration to ensure benefits are shared equally.\n","permalink":"http://localhost:1313/posts/myfirstpost/","summary":"\u003ch1 id=\"the-intelligence-age-by-sam-altman\"\u003eThe Intelligence Age by Sam Altman\u003c/h1\u003e\n\u003cp\u003eSam Altman explores the transformative power of AI in reshaping society. Key points include:\u003c/p\u003e\n\u003ch2 id=\"1-ais-future-potential\"\u003e1. AIâ€™s Future Potential\u003c/h2\u003e\n\u003cp\u003eAI will drive significant advances in fields like healthcare, education, and scientific research.\nIt promises to dramatically boost human productivity and quality of life.\u003c/p\u003e\n\u003ch2 id=\"2-challenges-ahead\"\u003e2. Challenges Ahead\u003c/h2\u003e\n\u003cp\u003eEquitable access to AI resources is crucial, requiring vast compute power and energy.\nThere are potential risks, especially concerning job displacement in the labor market.\u003c/p\u003e","title":"An introduction to Sam Altman's blog: The Intelligence Age"}]